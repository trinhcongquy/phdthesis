%% ====================================================================
\section{Linearizability}      
\label{section:specification:concurrent:data:structure}     
\index{Specification}%         
%% ====================================================================
\begingroup%     
In a concurrent program, the methods of the different executing threads can overlap in time. Thus, a {\tt rmv} method that
executes in parallel with an {\tt add} method for the same key may or may not
find the element in the set, depending on how the individual method statements
overlap in time. For a user of the data structure, it is important to know precisely
what can happen when several methods access a data structure concurrently,
without inspecting the code of each method. Such a user would want to have
a criterion for how operations take effect, which considers only the points in
time of method calls and returns. The most widely accepted such condition is
linearizability.  
%The program statements 
%in each method are totally ordered. Whereas, statements from different methods in different executing threads might form a partial order. This partial order raise the difficult of reasoning about program execution. One of the main correctness criterion of a concurrent program is linearizability, 
Linearizability defines consistency for the historyof call and response events generated by an execution of the program at hand \cite{HeWi:linearizability}. Intuitively, linearizability requires every method to take effectat some point ({\emph {linearization point}}) between its call and return events. A linearization point is intuitively a moment where the effect of the method
becomes visible to other threads. An execution of a (concurrent) system is modelled by a (concurrent) history, which is a finite
sequence of method call and return events. A (concurrent) history is linearizable if and only if there is some order forthe effects of the actions that corresponds to a valid sequential history. The valid sequence history can be generated by an execution of the sequential specification object. A concurrent object is linearizable iff each of its historiesis linearizable.

\setlength\intextsep{\dazintextsep}
\begin{figure}[ht]
  \centering
  \tikzinput[\linewidth]{img/linearizability} 
  \caption{Linearizability, where the linearization points are marked with \protect\commitpoint{}.}
  \label{figure:shape:linearizability}  
\end{figure}          
  

\index{Linearizability}%
Figure~\ref{figure:shape:linearizability} provides an example of a trace of method calls of a concurrent program implementing a set.
In the trace, each method takes effect
instantaneously at it is linearization point
between call and return events~\cite{HeWi:linearizability}. When we order methods according to their linearization points, we get a totally ordered sequence that respects the sequential specification of the set.
%A linearization point normally stays inside the code of the
%method.  However, in some cases, it it is located in
%the code of another method depending on the execution path.



\endgroup

%% ====================================================================
%\chapter{Verification of Linearizabilities}
\chapter{Specifying Linearizability}
In the previous sections, we described the correctness criterion of linearizability for concurrent data structures. In this section, we specify linearizability in a way that is suitable for automated verification.
We separate the problem of specifying linearizability into several ones.
\begin{itemize}

 \item  To specify the sequential semantics of a data structure in a way that is suitable for automated verification.
  \item To specify placement of linearization points in executions of a concurrent data structure.
  	 
\end{itemize}
  For the first, we use the techniques of observers \cite{AHHR:integrated:rep}. For the second, we present a new technique, in which methods are equipped with controllers. Controllers specify so-called ``linearization policies'', which prescribe how LPs are placed in executions.
  %You can also add a section, where you survey the technique of observers that synchronize at call and return events
 

\section{Observers}
\label{section:observers}
\index{Observers}
%% ====================================================================
%\bjcom{This introduction does not work. It is not an introduction to observers, but rather says how they are used in linearization policies. Some of the
%material can be moved to the survey section before the ``Observers'' section (see above)}
%
%In order to derive the totally ordered execution from a concurrent
%execution, each method is instrumented to generate a so-called
%abstract event whenever a linearization point is passed. Right after the program pass the linearization point, the abstract event is
%communicated to an external \emph{observer}, which records the sequences of
%abstract events from the code execution. In the next paragraph, we introduce the notion of observer, which essentially separates good traces of events from bad ones. Several observers shall be used to specify the safety property
Data structures are, by nature, infinite-state objects, since they are intended to carry an unbounded number of data elements. For automated verification, it is desirable with specifications that are constructed without explicitly mentioning such infinite objects.  This problem is addressed by observers \cite{AHHR:integrated:rep}. Observers specify allowed sequences of operations by constraining their projection on a small number of data elements. They exploit the assumption that the data structure handles all data elements in the same way.
% For instance, a stack data structure is specified by (* continue here *)
%We specify the sequencial \bjcom{please spell check!!!!} semantics of data structures by observers, as introduced
%in \cite{AHHR:integrated:rep}.
Observers are finite automata extended with a finite set of observer \emph{registers} that
assume values in the domain of data elements. At initialization, the registers are nondeterministically assigned arbitrary
values, which never change during a run of the observer. 
\begin{figure}[h]
%\begin{wrapfigure}{r}{0.5\textwidth} 
  \centering
  \tikzinput{img/set-observer} 
  \vspace{0.3cm}
  \caption{A set observer}
  \label{figure:shape:set:observers}
%\end{wrapfigure}
\end{figure}Transitions are labeled
by operations that may be parameterized on registers. 

Observers are used as
acceptors of sequences of operations on the data structure. The observer processes such sequences
one operation at a time. If there is a transition, whose label, after replacing registers by their values, matches the operation, such a transition is performed. If there is no such transition,
the observer remains in its current state. The observer accepts a sequence if it can be
processed in such a way that an accepting state is reached. The observer is defined in such a way that it accepts
precisely those sequences of abstract operations that are not allowed by the semantics of
the data structure. We use observers to give exact specifications of the behaviours of data structures such as sets, queues, and stacks. This is best illustrated by an example. Fig. ~\ref{figure:shape:set:observers} depicts an observer that
accepts the sequences of method invocations that are \emph{not} allowed by a sequential specification set data structure. The observer has three states $s_0$, $s_1$ and $s_2$. It also has one register which carries an arbitrary tracked data value. The initial state $s_0$ corresponds to positions in the runs where the non-deterministically tracked value
stored in the observer register $x$ is not present in the set (i.e.
each time it has been inserted it got deleted afterwards). The
state $s_1$ corresponds to positions in the runs where the tracked
value is present in the set (i.e, it has not been deleted since it
was last inserted). The accepting state $s_2$ corresponds to positions
in the runs where the non-allowed behavior captured by the observer has been observed. The captured bad specification are those where
a data value is deleted or found although it is not present in the set, or a data value is not found or cannot be deleted although it is already present in the set
%\bjcom{Continue and explain in 5-10 lines how it works}

\section{Linearization Policies}
\label{controllers:subsection}
In order to prove linearizability, the most intuitive approach is to find a linearization point (LP) in the code of the implementation, and show that it is the single point where the effect of the operation takes place \cite{AHHR:integrated:rep,BLMRS:cav08,Vafeiadis}.
\input lazy-list-rrules
%\bjcom{The preceding text in this paragraph can be moved and adapted to the earlier paragraphs of this chapter, where you explaine fixed and non-fixed LPs}
However, for a large class of linearizable implementations,
it is not possible to assign fixed LPs in the code of their methods, 
but depend on actions of other threads in each particular execution. For example, 
%\bjcom{A problem here is that the lazy set does not use ``helping''. PLease adapt the text to avoid such a misunderstanding}
in the Lazy Set algorithm, a successful {\tt rmv(e)} method has its LP at line 3, and an unsuccessful {\tt rmv(e)} has its LP at line 2 when the test $\tt c.val = e$ evaluates to \false. A successful {\tt add(e)} method has its LP at line 4 and an unsuccessful {\tt add} has its LP at line 2 when the test $\tt c.val != e$ evaluates to \false\;. The successful {\tt ctn} is linearized at line 4 then the value of $\tt b$ is \true. 
However it is not possible to assign a fixed LP in the code of the {\tt ctn} method when it is unsuccessful. 

To see why unsuccessful {\tt ctn} method invocations can not have fixed LPs, notethat the naive attempt of defining the LP at line 4 provided that the test at line 5 failswill not work. Namely, the {\tt ctn} method may traverse the list and arrive at line 4 in asituation where the element e is not in the list (either e is not in any cell, or the cellcontaining $\tt e$ is marked). However, before executing the command at line 4, anotherthread performs an add operation, inserting a new cell containing the element $\tt e$ intothe list. The problem is now that the {\tt ctn} method cannot “see” the new cell, since it isunreachable from the cell currently pointed to by the variable b of the {\tt ctn} method. Ifthe {\tt ctn} method would now try to linearize an unsuccessful {\tt ctn}, this would violatethe semantics of a set, since the add method just linearized a successful insertion of $\tt e$.
%However, linearizing an unsuccessful {\tt ctn(e)} is a bit tricky, and is a good example showing that it is not always
%possible to define a single linearization point for each method that works for all
%methods in all executions. In particular, simply choosing the linearization
%point for an unsuccessful {\tt ctn(e)} as the point at which a cell whose $\tt mark$ field is \true\; or a cell whose value of $\tt val$ is greater than the $\tt e$ (line 5) is found is
%incorrect. Consider the following case. Assume that a marked cell $\tt a$ where $\tt a.val = e$ is found and
%thread A is attempting to execute the method {\tt ctn(e)}. While A is traversing
%the list, $\tt c$ and all cells between $\tt c$ and $\tt a$ both are
%logically and physically removed. Thread A would still proceed until the point where $\tt c$ points to $\tt a$. Then, It detects that $\tt a$ is marked and therefore no longer in the list. It is correct to put its LP at this point in this case. However, consider what
%happens if while thread A is traversing the removed section of the list which leads to $\tt a$, another thread adds a new cell whose value of $\tt val$ is equal to $\tt e$ into the list. It is wrong to put LP at the point when $\tt c$ points to $\tt a$,
%since it occurs after the insertion of the new cell with same value of $\tt val$ a to the list. The method {\tt ctn(e)} is linearized at the earlier of the following points: (I) the point where a
%removed matching cell is found and (II) the point immediately before a new
%matching cell is added to the list. As can be seen, this linearization.
%
%\bjcom{The previous sentence is sloppy. You probably mean that ``it is not possible to assign a fixed LP in the code of the ctn method''. Your current text
%  may give the impression that there is an LP, but it is not fixed: it is unclear what this means}
%It stays in the code of the {\tt add} method.
%\bjcom{I suspect that the previous statement is incorrect. As I understand it, there is no fixed LP for ctn in the add method, since the placement depends on
%  what happens in the execution}
%\bjcom{Here, you must insert a detailed explanation of why this is the case, and how linearization points can be assigned in each execution. This will easily be
%10-20 lines. After that, end the paragraph}

 There have been several previous works dealing with the problems of non-fixed linearization points \cite{Poling,Colvin:Lazy-List,CGLM:cav06,SWD:cav12,Derrick:fm14,SDW:tcl14,Vafeiadis:cav10,Vafeiadis:Aspect}. However, they are either manual approaches without tool implementation or not general enough to cover various types of concurrent programs. 
 %\bjcom{You must comment in more detail about these works in a ``related works'' section of the intro}
 In this thesis we handle non-fixed linearization points by providing a mechanism for assigning LPs to executions, which we call \emph{linearization policies}. %The linearization point of a thread may be defined in two ways: \bjcom{Again, do not say that there are EXACTLY two ways}
%(i) The thread may define its own linearization point,
%and in that case may also help other threads define their 
%own linearization points.
%(ii) The thread may be helped by other threads.
%\bjcom{The points (i) and (ii) are almost impossible to understand, since you do not explain anything about what happens}
%\bjcom{I suggest you replace the preceding (i) and (ii) by giving a couple of EXAMPLES of how methods may linearize. You already have explained on
%  in the preceding paragraph (for lazy set). You can informally present a helping mechanism, which you can borrow from some other algorithm that
%  you verify. In all cases, you must explain concretely what happens in the concurrent algorithm, not just be abstract}
%The helping mechanism may contain complicated patterns.
%%

A linearization policy is expressed by associating method invocations to a \emph{controller}, which is responsible for generating operations announcing the occurrence of LPs during each method invocation. The controller is occasionally activated, either by its thread or by another controller, and mediates the interaction of the thread with theobserver as well as with other threads.

To add controllers, we first declare 
some statements in each method to be {\it triggering}: 
these are marked by the symbol \commitpoint,
as in Figure~\ref{figure:lazy-list}.
%
We specify the behaviour of the controller, belonging to a method
$\mname$, by a set  of
{\em reaction rules}. 
%
%
To define these rules, we first define different types of events
that are used to specify their behaviours.
%
Note that an {\it operation} is of the form $\mname(\indata,\outdata)$ where 
$\mname$ is a method name and $\indata$, $\outdata$ are data values.
Operations are emitted by the controller to the observer to notify 
that the thread executing the method performs a linearization of
the corresponding method with the given input and output values.
Next, we fix a set $\msgset$ of {\it broadcast messages},
each with a fixed  arity, which are used for
synchronization between controllers. A broadcast message is formed by supplying data
values as parameters. In reaction rules, these data values are denoted by
expressions over the variables of the method, which are evaluated in the
current state when the rule is invoked. We define two types of reaction rules: 
%In an operation, the first
%parameter, denoting input, must be either a constant
%or the parameter of the method call.

%% The set will be used to send and receive boradcast messages
%% to/from other threads.
%% %
%% A {\it send event} $\sevent$ is of the form
%% $\msg(\expr_1,\ldots,\expr_\nn)$ where 
%% $\msg\in\msgset$ is a message of arity $\nn$,
%% $\expr_1,\ldots,\expr_\nn$ are either 
%% local variables or constants.
%% %
%% %
%% We define $\msgof\sevent:=\msg$.
%% %
%% A {\it receive event} $\revent$ is of the form
%% $\msg(\xvar_1,\ldots,\xvar_\nn)$
%% where 
%% $\msg\in\msgset$ is a message with arity $\nn$, and
%% $\set{\xvar_1,\ldots,\xvar_\nn}$ is a set of variables (parameters)
%% that is joint from the set of local varibales of the thread.
%% %
%% We define $\msgof\revent:=\msg$.
%% %
%% An event (observer send or rceive) is said to be {\it ground} 
%% if all its arguments are constants.

\begin{itemize}
\item
  A {\em triggered} rule, of form
\(
%% \sndrule\cstate{\triggersym}\cnd\oevent\sevent{\cstate'}
\sndrulenostate{\triggersym}\cnd\oevent\sevent
\), 
specifies that whenever the method executes a triggering statement
and the condition $\cnd$ evaluates to {\tt true}, then 
the controller performs a {\em reaction} in which it emits the operation
obtained by evaluating $\oevent$
to the observer, and broadcasts the message obtained by evaluating $\sevent$
to the controllers of other threads.
%
%% Here $\cnd$ is a predicate on the set of local variables of the thread.
%
The broadcast message $\sevent$ is optional.
%\todo{We do not mention the object if it is not there.}
\item
  A {\em receiving} rule, of form
\(
\rcvrulenostate{\tuple{\revent,\ord}}\cnd\oevent
\),
specifies that whenever the observer of some other thread broadcasts
the message obtained by evaluating $\revent$, 
and $\cnd$ evaluates to {\tt true},
then the controller performs a
reaction where it emits the operation obtained by evaluating
$\oevent$ to the observer.
%
Note that no further broadcasting is
performed.
%
The interaction of the thread with the observer 
may occur either
{\it before} or {\it after}
the sender thread send the broadcast, according to the flag $\ord$.
\end{itemize}
%% In these rules, conditions $\cnd$, operations $\oevent$, and messages
%% $\sevent$, $\revent$ are expressions over the state accessible to the method;
%% they are evaluated in the current state when the rule is invoked.
A controller may also use a finite set of states,  which restrict the
possible sequences of reactions by a controller in the standard way.
Whenever such states are used, the rule includes source and target states
using kewords {\bf from} and {\bf goto}.
In Figure~\ref{rrules:lazy:list:fig}, the rule $\rulename_7$
changes the state from $q_0$ to $q_1$, meaning that no further applications
of rules $\rulename_6$ or $\rulename_7$ are possible, since they both
start from state $q_0$. Rules that do not mention states can be
applied regardless of the controller state and leave it unchanged.

Let us  illustrate how the reaction rules for controllers in
Figure~\ref{rrules:lazy:list:fig} specify LPs
for the algorithm in Figure \ref{figure:lazy-list}.
Here, a successful {\tt rmv} method has its LP at
line {\tt 3}, and
an unsuccessful {\tt rmv} has its LP at line {\tt 2} when the test
{\tt c.val = e} evaluates to {\tt false}.
Therefore, both these statements are marked as triggering.
The controller has a reaction rule for each of these cases: in
Figure~\ref{rrules:lazy:list:fig}:
rule $\rulename_3$ corresponds to a successful {\tt rmv},
whereas
rule $\rulename_4$ corresponds to an unsuccessful {\tt rmv}.
Rule $\rulename_4$ states that whenever the {\tt rmv}
method executes a
triggering statement, from a state where {\tt pc=2} and {\tt c.val != e}, then
the operation {\tt rmv(e,false)} will be emitted to the observer.

A successful {\tt add} method has its LP at line {\tt 4}. Therefore, the
controller for {\tt add} has the triggered rule $\rulename_1$ which emits
the operation {\tt add(e,true)} to the observer. In addition, the
controller
also broadcasts the message {\tt add(e)}, which is
received by any controller for a {\tt ctn} method which has not yet
passed line {\tt 4}, thereby
linearizing an unsuccessful {\tt ctn(e)} method by
emitting {\tt ctn(e,false)} to the observer.
  The keyword ${\bf b}$ denotes that the operation
  {\tt ctn(e,false)} will be presented before {\tt add(e,true)} to the observer.
Since the reception of {\tt add(e)} is performed in the same atomic step as the
triggering statement at line {\tt 4} of the {\tt add} method, this
describes a linearization pattern, where
a {\tt ctn} method, which has not yet reached line {\tt 4}, linearizes an
unsuccessful {\tt ctn}-invocation just before some other thread linearizes
a successful {\tt add} of the same element. 


\input img/policy 


 Let us describe an example of how the reaction rules in fig ~\ref{fig:policy} handles non-fixed linearization points of unsuccessful {\tt ctn(2)} method. Figure \ref{fig:policy} gives an example of Lazy Set with three threads $\tt T_1$, $\tt T_2$, and $\tt T_3$. The thread $\tt T_1$ is executing the {\tt add(2)} method to insert the cell whose value of $\tt val$ is equal to {\tt 2} into the set, while both $\tt T_2$ and $\tt T_3$ are executing {\tt ctn(2)} to search for a cell whose value of $\tt val$ is equal to {\tt 2}. When thread $\tt T_1$ reaches the triggering statement at line 4 of the {\tt add(2)} method at the step $\tt T_1$, the controller rule $\rulename_1$ is activated to inform the observer that an {\tt add} operation 
with argument {\tt 2} has been performed, and that the outcome
of the operation is {\tt true} (the operation was successful) in step $\tt 6$. However, before that, the controller will help other threads
to linearize. This is done by {\it broadcasting} a message {\tt add(2)} to the threads $\tt T_2$, $\tt T_3$ which are executing {\tt ctn(2)} in steps $\tt 2$. When these threads get the message then the rule $\rulename_7$ is activated to inform the observer that the element $\tt 2$ is not in the list in threads $\tt T_2$, $\tt T_3$ respectively, in steps $\tt 3$ and $\tt 5$ .
%\bjcom{The description of linearization policy in the preceding paragraph is in principle OK. But the grammar and language is not acceptable. There are even unfinished sentences. Please make an effort to write nice english, explaining in a way that colleagues can easily understand how things work.}
%
%\bjcom{Write more about linearization policies. You should by example explain syntax of controllers, intuitively what they can do, etc.
%  For instance, how they reduce checking for linearizability to checking (non)reachability of bad states. This includes all the stuff that is
%currently done by monitors.
%  You shoudl also survey results in the paper: e.g., later you write ``In paper II, we show that ... reduces to reachability ...''}
\closeparagraph{Verifying Linearization Policies} By using an observer to specify the sequential semantics of the
data structure, and defining controllers that specify the linearization policy,
the verification of linearizability is reduced to establishing four conditions:
\begin{inparaenum}[(i)]
\item
  each method invocation generates a non-empty sequence of operations,
\item
  the last operation of a method conforms to its parameters and return value,
\item
  only the last operation of a method may
  change the state of the observer,
  and
\item
  the sequence of all operations
  cannot drive the observer to an accepting state.
\end{inparaenum}
Our verification framework automatically reduces
the establishment of these conditions to 
a problem of checking control state reachability. This is done by
augmenting the observer by a {\em monitor}. The monitor is automatically
generated. It keeps track of 
%% The monitor 
the state of the observer, and records the sequence of operations and
call and return actions generated by the threads.
For each thread, it keeps track of whether
it has linearized, whether it has caused a state change in the observer, and
the parameters used in its last linearization. Using this information, it
goes to an error state whenever any of the above four conditions is violated.


